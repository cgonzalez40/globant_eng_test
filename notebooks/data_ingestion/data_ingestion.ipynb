{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f2b0e18-8492-45b2-bf41-e45151692e37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import needed libraries\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc20b180-8d82-4d2c-87c7-49d37eca8901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining global variables\n",
    "project_id = \"poc-accionclimatica-agrilac\"\n",
    "schema_name =  \"training\"\n",
    "# Test variable\n",
    "gcs_location = \"gs://datalake_gb/zone=landing/data=departments/departments.csv\"\n",
    "schema_fields = [\"id\",\"department\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76f8968d-d89f-4fa0-bfb5-ed3bc29f6449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pandas(url:str,\n",
    "                chunk_size = 1000,\n",
    "               schema_fields = []):\n",
    "    \"\"\"\n",
    "\n",
    "    Load a CSV file into a Pandas DataFrame using chunking.\n",
    "\n",
    "    This function reads a CSV file from the given URL in chunks and \n",
    "    allows specifying column names and chunk size.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    url : str\n",
    "        The URL or file path of the CSV file to be loaded.\n",
    "    chunk_size : int, optional (default=1000)\n",
    "        The number of rows per chunk when reading the CSV file.\n",
    "    schema_fields : list, optional (default=[])\n",
    "        A list of column names to be used as headers. If empty, \n",
    "        the default headers from the file are used.\n",
    "    \"\"\"\n",
    "    return  pd.read_csv(url,\n",
    "                        chunksize=chunk_size,\n",
    "                       sep=\",\",\n",
    "                       names = schema_fields,\n",
    "                       header = 0\n",
    "                       )\n",
    "\n",
    "def bigquery_insert(df:pd.DataFrame, table:str):    \n",
    "    \"\"\"\n",
    "    Inserts a Pandas DataFrame into a specified BigQuery table.\n",
    "\n",
    "    This function uploads a DataFrame to a BigQuery table using the \n",
    "    BigQuery Python client. It assumes that the table schema is already defined.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The DataFrame containing the data to be inserted into BigQuery.\n",
    "    table : str\n",
    "    The fully qualified name of the target BigQuery table in the format `project.dataset.table`.\n",
    "\n",
    "    \"\"\"\n",
    "    client = bigquery.Client()\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        write_disposition=\"WRITE_APPEND\",\n",
    "    )\n",
    "    job = client.load_table_from_dataframe(\n",
    "        df, table, job_config=job_config\n",
    "    )\n",
    "    job.result()\n",
    "        \n",
    "def main(gcs_location:str,\n",
    "         schema_fields:[]):\n",
    "    \"\"\"\n",
    "    Main function to process data from a GCS (Google Cloud Storage) location \n",
    "    and insert it into BigQuery.\n",
    "\n",
    "    This function:\n",
    "    1. Reads a CSV file from the specified GCS location using `load_pandas`.\n",
    "    2. Extracts the table name from the GCS file path.\n",
    "    3. Formats the BigQuery table reference.\n",
    "    4. Inserts the data into BigQuery using `bigquery_insert`.\n",
    "    \"\"\"\n",
    "    data = load_pandas(url=gcs_location,\n",
    "                   schema_fields = schema_fields )\n",
    "    for _data in data:\n",
    "        sample = _data\n",
    "        _table_name = (gcs_location.split(\"/\")[-2]).split(\"=\")[1]\n",
    "        formated_table = f\"{project_id}.{schema_name}.{_table_name}\"\n",
    "        bigquery_insert(_data,formated_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac2f3ce2-1246-44e1-b0d5-c0a397f8edb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "llm",
   "name": "workbench-notebooks.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m118"
  },
  "kernelspec": {
   "display_name": "llm (Local)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
